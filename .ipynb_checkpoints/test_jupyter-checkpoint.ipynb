{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "import requests\n",
    "import sys\n",
    "import json\n",
    "import time   # get the current time\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime  # format the news issue time, and compare with the current time\n",
    "from pymongo import MongoClient\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "headers = {\n",
    "\t\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/64.0.3282.167 Chrome/64.0.3282.167 Safari/537.36\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News_basic():\n",
    "\tdef __init__(self, title, docurl, commenturl, n_id, n_time):\n",
    "\t\tself.title = title\n",
    "\t\tself.docurl = docurl\n",
    "\t\tself.commenturl = commenturl\n",
    "\t\tself.n_id = n_id\n",
    "\t\tself.n_time = n_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_page_url(url):\n",
    "\ttry:\n",
    "\t\tpage_urls = []\n",
    "\t\tfor i in range(0, 10):\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tpage_urls.append(url+\"cm_war.js\")\n",
    "\t\t\telif i > 1 and i < 4:\n",
    "\t\t\t\tpage_urls.append(url+\"cm_war_\"+str(\"%02d\"%i)+\".js\")\n",
    "\t\treturn page_urls\n",
    "\texcept Exception as e:\n",
    "\t\tprint \"From:crawl_page_url\\n\\tUnexpect Error: {}\".format(e)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news_url(page_urls):\n",
    "\t\n",
    "\ttry:\n",
    "\t\tnews_urls = []\n",
    "\t\tfor url in page_urls:\n",
    "\t\t\t# print url\n",
    "\t\t\tr = requests.get(url, headers = headers)\n",
    "\t\t\tif r.status_code == 200:\n",
    "\t\t\t\tt1 = r.text \n",
    "\t\t\t\traw_json = t1.strip(\"data_callback(\").rstrip(')')\n",
    "\t\t\t\tj = json.loads(raw_json)\n",
    "\t\t\t\tfor sub_url in j:\n",
    "\t\t\t\t\ttime_state = 1\n",
    "\t\t\t\t\tn_id = sub_url['commenturl'].split('/')[-1].split('.')[0]\n",
    "\t\t\t\t\tif sub_url['time'] != \"\":\n",
    "\t\t\t\t\t\traw_time = sub_url['time'].split()  \t\t\t\t\t# defautl the sparate sign is space\n",
    "\t\t\t\t\t\traw_time1 = raw_time[0].split('/')\n",
    "\t\t\t\t\t\traw_time2 = raw_time[1].split(':')\n",
    "\t\t\t\t\t\tn_time = datetime(int(raw_time1[2]), int(raw_time1[0]), int(raw_time1[1]), int(raw_time2[0]), int(raw_time2[1]), int(raw_time2[2]))\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttime_state = 0\n",
    "\t\t\t\t\t\tprint sub_url['docurl']\n",
    "\t\t\t\t\t\tprint \"Note --- No time value!\"\n",
    "\t\t\t\t\tnews = News_basic(sub_url['title'], sub_url['docurl'], sub_url['commenturl'], n_id, n_time)    #\"time\":\"03/31/2018 15:14:01\"\n",
    "\t\t\t\t\tpp = datetime(2018,1,1)  # According observation, I found the js number great than or equal 02 is too early news\n",
    "\t\t\t\t\tif n_time < pp:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\tif sub_url['docurl'].split('/')[2] == \"war.163.com\" or sub_url['docurl'].split('/')[2] == \"dy.163.com\":\n",
    "\t\t\t\t\t\tif news not in news_urls and time_state == 1:    # you can judge a dict whether in the list \n",
    "\t\t\t\t\t\t\tnews_urls.append(news)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise Exception(\"The return code is %d\"%r.status_code)\n",
    "\t\treturn news_urls\n",
    "\texcept Exception as e:\n",
    "\t\tprint \"From:crawl_news_url:\\n\\tUnexpect Error: {}\".format(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_time(days):\n",
    "\ttry:\n",
    "\t\tc_t = time.localtime(time.time())\n",
    "\t\t#process the date, only crawl the five days ago news\n",
    "\t\tc_y = c_t[0]\n",
    "\t\tc_m = c_t[1]\n",
    "\t\tc_d = c_t[2]\n",
    "\n",
    "\t\tms = days/30\n",
    "\t\tys = ms/12\n",
    "\t\tdays = days-ms*30\n",
    "\t\tms = ms-ys*12\n",
    "\n",
    "\t\tif c_d - days > 0:\n",
    "\t\t\tc_d -= days\n",
    "\t\telse:\n",
    "\t\t\tms += 1\n",
    "\t\t\tc_d = 30-days+c_d\n",
    "\n",
    "\t\tif c_m - ms > 0:\n",
    "\t\t\tc_m -= ms\n",
    "\t\telse:\n",
    "\t\t\tys += 1\n",
    "\t\t\tc_m = 12-ms+c_m\n",
    "\t\tc_y -= ys\n",
    "\t\treturn datetime(c_y, c_m, c_d)\n",
    "# Version2: get current time\n",
    "\t# datetime.utcnow()\n",
    "\texcept Exception as e:\n",
    "\t\tprint \"From:get_point_time\\n\\tUnexpect Error: {}\".format(e)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news_content(docurl, n_id):\n",
    "\t\n",
    "\tkeywords =''\n",
    "\tcontent = ''\n",
    "\ttry:\n",
    "\t\tr = requests.get(docurl, headers = headers)\n",
    "\t\tt = r.text\n",
    "\t\tif r.status_code == 200:\n",
    "\t\t\tsoup = BeautifulSoup(t, 'html.parser')\n",
    "\t\t\t\n",
    "\t\t\tfor i in soup.find_all('meta'):\n",
    "\t\t\t\tif i.get('name') == 'keywords':\t\t\t\t\t\t\t\t# *****note the article not keywords\n",
    "\t\t\t\t\tkeywords = [word for word in i['content'].split(',')]\n",
    "\n",
    "\t\t\t#There have three types file, \n",
    "\t\t\t#1, war.163   2. photoview  3. article\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif docurl.split('/')[3] == 'photoview':\n",
    "\t\t\t\t\ttext = json.loads(soup.textarea.get_text())\n",
    "\t\t\t\t\tcontent = text['info']['prevue']\n",
    "\t\t\t\t\tfor t in text['list']:\n",
    "\t\t\t\t\t\tcontent = content + t['note']\n",
    "\t\t\t\telif docurl.split('/')[4] == 'article':\n",
    "\t\t\t\t\tcontent = soup.find('div', class_ = 'content').get_text()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcontent = soup.find('div', class_ = 'post_text').get_text()\n",
    "\t\t\t\tcontent = content.strip()\t\t\t\t# clean the date\n",
    "\t\t\t\tcontent = content.replace('\\n', '')\n",
    "\t\t\t\tcontent = content.replace('<br>', '')\n",
    "\n",
    "\t\t\t\t# return 1, content, keywords\n",
    "\t\t\t\treturn content, keywords\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint \"Unexpect Error: {}\".format(e)\n",
    "\t\t\t\tcontent = -1\n",
    "\t\t\t\tkeywords = -1\n",
    "\t\t\t\treturn content, keywords\n",
    "\t\t\t\t# return -1, content, keywords  # draw the content failed!\n",
    "\t\telse:\n",
    "\t\t\traise Exception(\"The return code is %d---crawl_news_content\"%r.status_code)\n",
    "\n",
    "\texcept Exception as e:   # set a error type *********************************\n",
    "\t\tprint \"The %s article:\"%n_id\n",
    "\t\tprint \"From:crawl_news_content:\\n\\tUnexpect Error: {}\".format(e)\n",
    "\t\treturn -2, content, keywords # request failed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news_cmtcount(commenturl, n_id):\n",
    " \tcmt_url = commenturl\n",
    " \tcmt_id = n_id\n",
    "\tbase_url = \"http://sdk.comment.163.com/api/v1/products/a2869674571f77b5a0867c3d71db5856/threads/\"\n",
    "\tcount_url = base_url + cmt_id\n",
    "\ttry:\n",
    "\t\tr = requests.get(count_url, headers = headers)\n",
    "\t\tif r.status_code == 200:\n",
    "\t\t\tj = json.loads(r.text)\n",
    "\t\t\ttie_count = j['tcount']\n",
    "\t\t\tjoin_count = j['cmtVote'] + j['cmtAgainst'] + j['rcount']\n",
    "\t\t\treturn tie_count\n",
    "\t\t\t# return 1, tie_count  # success\n",
    "\t\telse:\n",
    "\t\t\traise Exception(\"The return code is %d---crawl_news_cmtcount\"%r.status_code)\n",
    "\t\t\t# return -1\n",
    "\t\t\t# return -2, -1  # request failed !\n",
    "\texcept Exception as e:\n",
    "\t\tprint \"The %s article:\"%n_id\n",
    "\t\tprint \"From:crawl_news_cmtcount:\\n\\tUnexpect Error: {}\".format(e)\n",
    "\t\treturn -1\n",
    "\t\t# return -3, -1\t# request failed, avoid the process stop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news_comment(commenturl, tie_count, n_id):\n",
    "\ttry:\n",
    "\t\t# draw the news number through the news comment url\n",
    "\t\tcmt_list = []\n",
    "\t\tcmt_id_list = []\n",
    "\t\tcmt_id = n_id\n",
    "\n",
    "\t\tcmt_url = commenturl\n",
    "\t\tbase_url = \"http://sdk.comment.163.com/api/v1/products/a2869674571f77b5a0867c3d71db5856/threads/\"\n",
    "\t\tc_url = base_url + cmt_id + \"/comments/newList\"\n",
    "\t\tpayload = {'offset': 0, 'limit': 30}\n",
    "\n",
    "\t\t# state = 1  # represent the normal return value\n",
    "\t\twhile(payload['offset'] < tie_count):\n",
    "\t\t\t# try:\n",
    "\t\t\tr = requests.get(c_url, params = payload)\n",
    "\t\t\t# print r.status_code\n",
    "\t\t\tif r.status_code == 200:\n",
    "\t\t\t\tj = json.loads(r.text)\n",
    "\t\t\t\tif(len(j['comments']) > 0):\n",
    "\t\t\t\t\t# OR for k in j['comments']:\n",
    "\t\t\t\t\tfor k,v in j['comments'].items():   # if you wanna add more info, you can add info to the cmt_info\n",
    "\t\t\t\t\t\tcmt_info = {}\n",
    "\t\t\t\t\t\tif j['comments'][k]['commentId'] not in cmt_id_list:\n",
    "\t\t\t\t\t\t\tcmt_id_list.append(j['comments'][k]['commentId'])\n",
    "\t\t\t\t\t\t\tcmt_info['commentId'] = j['comments'][k]['commentId']\n",
    "\t\t\t\t\t\t\tcmt_info['content'] = j['comments'][k]['content']\n",
    "\t\t\t\t\t\t\tcmt_info['vote'] = j['comments'][k]['vote']\n",
    "\t\t\t\t\t\t\tcmt_info['against'] = j['comments'][k]['against']\n",
    "\t\t\t\t\t\t\tc_t = j['comments'][k]['createTime'].split()\t\t# time type 2018-04-08 07:28:00\n",
    "\t\t\t\t\t\t\tc_t_0 = c_t[0].split('-')\n",
    "\t\t\t\t\t\t\tc_t_1 = c_t[1].split(':')\n",
    "\t\t\t\t\t\t\tcmt_info['date'] = datetime(int(c_t_0[0]), int(c_t_0[1]), int(c_t_0[2]), int(c_t_1[0]), int(c_t_1[1]), int(c_t_1[2]))\n",
    " \n",
    "\t\t\t\t\t\t\tcmt_info['content'] = cmt_info['content'].replace('<br>', '')  #clean the  data\n",
    "\n",
    "\t\t\t\t\t\t\tcmt_list.append(cmt_info)    # the cmt_list include cmt_info dictory\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbreak  # Quit while\n",
    "\t\t\t\tpayload['offset'] = payload['offset'] + 30\n",
    "\t\t\telse:\n",
    "\t\t\t\t# state = state + 1  # error\n",
    "\t\t\t\t# payload['offset'] = payload['offset'] + 30\n",
    "\t\t\t\t# print \"status_code error --- crwal the comment failed!\"\n",
    "\t\t\t\traise Exception(\"The return code is %d---crawl_news_comment\"%r.status_code)\n",
    "\t\t\t\t# continue\n",
    "\t\t\t# except Exception as e:\n",
    "\t\t\t# \tstate = state + 1 # error\n",
    "\t\t\t# \tpayload['offset'] = payload['offset'] + 30\n",
    "\t\t\t# \t# print \"Crawl the comment failed!\"\n",
    "\t\t\t# \tprint \"The %s article:\"%n_id\n",
    "\t\t\t# \tprint \"From:crawl_news_comment:\\n\\tUnexpect Error: {}\".format(e)\n",
    "\t\t\t# \tcontinue\n",
    "\t\treturn cmt_list\n",
    "\t\t# return state, cmt_list\n",
    "\texcept Exception as e:\n",
    "\t\tprint \"The %s article:\"%n_id\n",
    "\t\tprint \"From:crawl_news_comment:\\n\\tUnexpect Error: {}\".format(e)\n",
    "\t\treturn -1\t#Error Return -1, occur the mongodb the comments value is -1 (Don't why(have continue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling(news_urls, tag_time):\n",
    "\ttry:\n",
    "\t\ttry:\n",
    "\t\t\tconn = MongoClient('127.0.0.1', 27017)\n",
    "\t\t\tdb = conn.netease\n",
    "\t\t\twar = db.war\n",
    "\t\texcept:\n",
    "\t\t\tprint \"Connect mongodb failed!\"\n",
    "\t\t\treturn \t\t\t\t\t\t\t\t# equal return None\n",
    "\n",
    "\t\tnew_insert = 0\n",
    "\t\tnew_update = 0\n",
    "\n",
    "\t\tif news_urls and len(news_urls) > 0: # if news_urls length is greate than 0 or news_urls is not None\n",
    "\t\t\tfor news in news_urls:\n",
    "\t\t\t\texist = 0\n",
    "\t\t\t\ts_tie_count = 0\n",
    "\t\t\t\tif news.n_time > tag_time:\n",
    "\t\t\t\t\t# print news.n_id\n",
    "\t\t\t\t\t# print news.n_time\n",
    "\t\t\t\t\tif war.find({\"number\":news.n_id}).count() > 0 :\t\t# the news have been stored!\n",
    "\t\t\t\t\t\t\texist = 1\n",
    "\t\t\t\t\t\t\ts_tie_count = war.find_one({'number': news.n_id})['tie_count']\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t# print \"NNN --- The news have stored in mongodb!\"\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcontent, keywords = crawl_news_content(news.docurl, news.n_id)\n",
    "\n",
    "\t\t\t\t\ttie_count = crawl_news_cmtcount(news.commenturl, news.n_id)\n",
    "\t\t\t\t\tif tie_count == -1:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t\t\t\tif exist == 1 and s_tie_count == tie_count or tie_count == 0:  # No the latest comments \n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t# print \"No the latest comments! And no update Attribute!\"\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcmt_list = crawl_news_comment(news.commenturl, tie_count, news.n_id)\n",
    "\t\t\t\t\t\tif cmt_list == -1:\n",
    "\t\t\t\t\t\t\tcmt_list = []\n",
    "\t\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tif exist == 0:\n",
    "\t\t\t\t\t\tnews_item = {}\n",
    "\t\t\t\t\t\tnews_item['number'] = news.n_id\n",
    "\t\t\t\t\t\tnews_item['title'] = news.title\n",
    "\t\t\t\t\t\tnews_item['docurl'] = news.docurl\n",
    "\t\t\t\t\t\tnews_item['commenturl'] = news.commenturl\n",
    "\t\t\t\t\t\tnews_item['date'] = news.n_time\n",
    "\t\t\t\t\t\tnews_item['keywords'] = keywords\n",
    "\t\t\t\t\t\tnews_item['content'] = content\n",
    "\t\t\t\t\t\tnews_item['tie_count'] = tie_count\n",
    "\t\t\t\t\t\tnews_item['comments'] = cmt_list\n",
    "\n",
    "\t\t\t\t\t\twar.insert_one(news_item)\n",
    "\t\t\t\t\t\tnew_insert += 1\n",
    "\t\t\t\t\t\t# print \"The news insert successed !\"\n",
    "\t\t\t\t\telif exist == 1 and s_tie_count != tie_count:\n",
    "\t\t\t\t\t\twar.update_one({'number': news.n_id}, {'$set':{'tie_count': tie_count, 'comments': cmt_list}})\n",
    "\t\t\t\t\t\tnew_update += 1\n",
    "\t\t\t\t\t\t# print \"The news update successed !\"\n",
    "\t\t\t\t\telif exist == 1 and s_tie_count == tie_count:\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t# print \"Do not need update !\"\n",
    "\t\t\t\t# else:\n",
    "\t\t\t\t# \tprint \"The news is more than five days ago!\"\n",
    "\t\telse:\n",
    "\t\t\tprint \"No the url to be crawl!\"\n",
    "\t\tconn.close()\n",
    "\t\treturn new_insert, new_update\n",
    "\texcept Exception as e:\n",
    "\t\tprint \"From:crawling:\\n\\tUnexpect Error: {}\".format(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cmts():\t\t\t\t# ------   zeng liang pa qu comments and using mongodb update to add the new comments\n",
    "\t\t\t\t\t\t\t\t# ------\tso that can saving the time of crawling the comments\n",
    "\ttry:\n",
    "\t\tprint \"updating the comments for stored article......\"\n",
    "\t\ttag_time = get_point_time(1)\n",
    "\t\tconn = MongoClient('127.0.0.1', 27017)\n",
    "\t\tdb = conn.netease\n",
    "\t\twar = db.war\n",
    "\t\tnew_update = 0\n",
    "\t\tfor i in war.find():\n",
    "\t\t\tif i['date'] > tag_time:\n",
    "\t\t\t\ts_tie_count = i['tie_count']\n",
    "\t\t\t\ttie_count = crawl_news_cmtcount(i['commenturl'], i['number'])\n",
    "\t\t\t\tcmt_list = crawl_news_comment(i['commenturl'], tie_count, i['number'])\n",
    "\t\t\t\tif tie_count != s_tie_count and tie_count != -1 and cmt_list != -1:\n",
    "\t\t\t\t\tnew_update += 1\n",
    "\n",
    "\t\tconn.close()\n",
    "\t\treturn new_update\n",
    "\texcept Exception as e:\n",
    "\t\tprint \"From:update_cmts:\\n\\tUnexpect Error: {}\".format(e)\n",
    "\n",
    "def crawl_new(n):\n",
    "\turl = \"http://temp.163.com/special/00804KVA/\"\n",
    "\tprint \"Crawling page url......\"\n",
    "\tpage_urls = crawl_page_url(url)\n",
    "\tprint \"Crawling news url......\"\n",
    "\tnews_urls = crawl_news_url(page_urls)\n",
    "\n",
    "\ttag_time = get_point_time(n)\t\t# get five days ago time\t\n",
    "\n",
    "\tprint \"Crawling news......\"\n",
    "\t# All_update = 0   # whether update all \n",
    "\tnew_insert, new_update = crawling(news_urls, tag_time)\n",
    "\tprint \"Insert %d article\\nUpdate %d article comments\"%(new_insert, new_update)\n",
    "\treturn new_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "\turl = \"http://temp.163.com/special/00804KVA/\"\n",
    "\tprint \"Crawling page url......\"\n",
    "\tpage_urls = crawl_page_url(url)\n",
    "\tprint \"Crawling news url......\"\n",
    "\tnews_urls = crawl_news_url(page_urls)\n",
    "\n",
    "\ttag_time = get_point_time(1)\t\t# get five days ago time\t\n",
    "\n",
    "\tprint \"Crwaling news......\"\n",
    "\t# All_update = 0   # whether update all \n",
    "\tnew_insert, new_update = crawling(news_urls, tag_time)\n",
    "\tprint \"Insert %d article\\nUpdate %d article comments\"%(new_insert, new_update)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
